# This code has generated by ChatGPT for the most part
# It has then been reviewed, corrected, commented and augmented by
# a humabn being (Eric).

import random
import heapq
import numpy as np

# This model represents a memory hierarchy with
# - 3 levels of cache (L1, L2, L3)
# - a DDR memory implemening some simple optimization features (see the DDR class)
# - an interconnect
# The number of cores, levels of cache, characteristics of the cache (number of ways,...)
# are parameters and can be modified.



# -----------------------------------------------------
# CacheLine: Represents a single cache line in the cache hierarchy
# -----------------------------------------------------
# We are using n-way associative cache:
# - Each set contains n cache lines with a tag entry.
# - Each line contains m bytes.
#
# A memory Address is structured as follows:
# +----------------+-----------+-----------+
# |     Tag        |   Index   |  Offset   |
# +----------------+-----------+-----------+
# where
#
# (Used to find blk) (Set selection) (Byte in block)
# "Block" and "line" are used interchangeably.
# Cache Structure:
# Set 0: [Block 0] [Block 1] [Block 2] [Block 3]  ← 4-way associativity (n=4)
# Set 1: [Block 0] [Block 1] [Block 2] [Block 3]
# ...
# Set N: [Block 0] [Block 1] [Block 2] [Block 3]

class CacheLine:
    def __init__(self):
        self.valid = False       # Indicates if this line holds valid data
        self.tag = None          # Tag of the data block
        self.data = 0            # Simulated data content
        self.dirty = False       # Indicates if the line has been written to (for write-back)

# Implements Pseudo-LRU (PLRU) replacement policy for N-way set associative caches
# The pseudoi-LRU is used to determine the bock to replace in case of cache miss.
# A binary tree is used to implement the PLRU algorithm. There is one tree per set.
# For a 4-way cache, 3 bits are used to determine the block to select.
#
#      Bit 0 (Root)
#     /           \
#   Bit 1         Bit 2
#   /   \         /   \
# Block0 Block1 Block2 Block3
#
# Each node of the tree contains a direction (left=0, right=1) that indicates
# the path to follow to find the next pLRU entry.
class PLRU:
    def __init__(self, ways):
        self.bits = [0] * (ways - 1)  # Tree structure to track usage
        self.ways = ways

    # Update the binary tree in case of a hit
    # The bits in the tree are modified to point "away" from this entry
    # (which is the MRU)
    def update_on_access(self, way):
        idx = 0
        num_levels = self.ways.bit_length() - 1
        for level in range(num_levels):
            # Select direction according to the way
             # (e.g., way=3=0b101 in a 4-way cache => direction = 1 (right subtree), 0 (left subtree)
            direction = (way >> (num_levels - 1 - level)) & 1
            self.bits[idx] = 1-direction # Point to the opposite direction
            idx = (idx << 1)+ 1 + direction

    # Compute the next victim (the pLRU)
    # The block is selected by traversing the tree according
    # to the directions given by each bit.
    def get_victim(self):
        idx = 0
        way = 0
        for level in range(self.ways.bit_length() - 1):
            direction = self.bits[idx]
            way = (way << 1) | direction
            idx = ( idx << 1) + 1 + direction
        return way

# ---------------------------------------------------------
# Represents a memory access request (either read or write)
# ---------------------------------------------------------
class DDRRequest:
    def __init__(self, core_id, time, req_type, addr, callback=None, value=None, num_instr=0):
        self.core_id = core_id
        self.time = time              # Time of request
        self.req_type = req_type      # 'read' or 'write'
        self.addr = addr
        self.callback = callback      # Callback function to return read result
        self.value = value            # Value to write (for write requests)
        self.num_instr = num_instr

    def __lt__(self, other):
        return self.time < other.time

# ---------------------------------------------------------
# Interconnect model between CPU cores and DDR, with bandwidth and latency
# ---------------------------------------------------------
# Behaviour :
# - Each request takes at least some base delay to be served.
# - A request may be delayed if the interconnect bandwidh has been "used"
# The interconnect canot serve more than "bandwidth" request in one cycle.
# Note
# - Using a heapqueue ensures that all items are and remain sorted
#   according to their ready_time (and req)
#


class Interconnect:
    def __init__(self, memory, delay=5, bandwidth=4):
        self.memory = memory
        self.queue = []               # Queue of pending memory requests
        self.delay = delay            # Base delay before forwarding to DDR
        self.bandwidth = bandwidth    # Max number of requests per cycle
        self.cycle = 0

    # Push a request in the request queue.
    # We push the couple (ready_time, request) where ready_time is the earliest
    # time at which the request may be served.
    def request(self, req):
        #print(f"[Cycle {self.cycle}] ➜ New CPU request queued: Core {req.core_id}, {req.req_type.upper()} Addr {req.addr}")
        ready_time = self.cycle + self.delay + random.randint(0, 2)
        #print("ready time", ready_time, "req", req.addr, "core",req.core_id)
        heapq.heappush(self.queue, (ready_time, req))

    # Process the interconnect current cycle
    def tick(self):
        processed = 0
        temp_queue = []

        # Forward requests to DDR respecting bandwidth limit
        while self.queue and self.queue[0][0] <= self.cycle and processed < self.bandwidth:
            _, req = heapq.heappop(self.queue)

            #print(f"[Cycle {self.cycle}] Interconnect Forwarding: Core {req.core_id} {req.req_type.upper()} addr {req.addr}")
            self.memory.request(req)
            processed += 1

        #[TODO]: the following code seems useless since the queue is always
        #sorted...

        # Defer excess requests for next cycles
        while self.queue and self.queue[0][0] <= self.cycle:
            temp_queue.append(heapq.heappop(self.queue))
            #print(f"[Cycle {self.cycle}] Interconnect Saturated: Deferring {len(self.queue)} requests")
        for item in temp_queue:
            heapq.heappush(self.queue, item)

        self.cycle += 1


#---------------------------------------
# Models one level in the cache hierarchy
#----------------------------------------
class CacheLevel:
    miss_history = {"level":[],"addr":[],"core_id":[]}
    num_instr = 0
    def __init__(self, level_name, core_id, size, line_size, assoc, memory=None, write_back=True, write_allocate=True):
        self.level = level_name
        self.core_id = core_id
        self.line_size = line_size
        self.assoc = assoc
        self.num_sets = (size // line_size) // assoc
        self.sets = [[CacheLine() for _ in range(assoc)] for _ in range(self.num_sets)]
        self.plru_trees = [PLRU(assoc) for _ in range(self.num_sets)]
        self.memory = memory        # Could be DDR or next cache level
        self.lower = None           # Lower level cache
        self.write_back = write_back
        self.write_allocate = write_allocate
        self.hits = 0
        self.misses = 0
        self.num_instr =0
        #self.
        #self.num_instr = 0

    # Extract the set index from the address
    #  addr = [ tag ][ idx ][ offset ]
    def _index(self, addr):
        return (addr // self.line_size) % self.num_sets

    # Extract the tag from the address
    #  addr = [ tag ][ idx ][ offset ]
    def _tag(self, addr):
        return addr // (self.line_size * self.num_sets)

    # Handles cache read request
    def read(self, addr, callback):
        index = self._index(addr)
        tag = self._tag(addr)
        cache_set = self.sets[index]
        plru = self.plru_trees[index]
        out = 0
        if self.level=="L3":
            self.num_instr +=1
        # Seach the tag in the cache set
        for i, line in enumerate(cache_set):
            if line.valid and line.tag == tag:
                # There is a hit.
                #print(f"[{self.level} cache hit for {addr} on core {self.core_id}]")
                self.hits += 1
                # Update the pLRU tree to point away from the MRU
                plru.update_on_access(i)
                callback(line.data)
                return out

        #print(f"[{self.level} cache miss for {addr} on core {self.core_id}]")
        self.miss_history["level"].append(self.level)
        self.miss_history["addr"].append(addr)
        self.miss_history["core_id"].append(self.core_id)
        # On miss, choose victim line using PLRU and fetch from lower memory
        self.misses += 1
        victim_idx = plru.get_victim()
        victim_line = cache_set[victim_idx]

        # If the victime line is valid and dirty, we have to write the data to
        # the next level of memory before loading the cache entry with the
        # data.
        def lower_cb(val):
            # Write evicted data if dirty
            if victim_line.valid and victim_line.dirty and self.write_back:
                victim_addr = ((victim_line.tag * self.num_sets) + index) * self.line_size
                if self.lower:
                    self.lower.write(victim_addr, victim_line.data)
                elif self.memory:
                    self.memory.request(DDRRequest(self.core_id, self.memory.cycle, 'write', victim_addr, value=victim_line.data))
            # Now that we have written the data to the next memory level, the
            # cache entry is updated with the new data (val)
            victim_line.valid = True
            victim_line.tag = tag
            victim_line.data = val
            victim_line.dirty = False
            plru.update_on_access(victim_idx)
            callback(val)

        # Forward the read request to the lower-level cache (if any)
        if self.lower:
            out = self.lower.read(addr, lower_cb)
#            return 1
        # Or to DDR...
        elif self.memory:
            self.memory.request(DDRRequest(self.core_id, self.memory.cycle, 'read', addr, lower_cb,num_instr=self.num_instr))
            out = 1 #acces to ddr
#            return 1
        return out

    # Handles cache write request
    def write(self, addr, val):
        index = self._index(addr)
        tag = self._tag(addr)
        cache_set = self.sets[index]
        plru = self.plru_trees[index]
        #print(f"[{self.level} cache write for {addr} on core {self.core_id}]")
        out = 0
        if self.level=="L3":
            self.num_instr +=1
        for i, line in enumerate(cache_set):
            if line.valid and line.tag == tag:
                # There is a cache hit
                line.data = val
                # If the cache is write-back, the data will be written to
                # memory when evicted, so it is marked "dirty"
                line.dirty = True if self.write_back else False
                plru.update_on_access(i)
                # If the cache is write-through, the write operation is
                # propagated to the lower levels of the memory hierarchy
                if not self.write_back:
                    if self.lower:
                        self.lower.write(addr, val)
                    elif self.memory:
                        self.memory.request(DDRRequest(self.core_id, self.memory.cycle, 'write', addr, value=val))
                        out = 1
                return out

        # There is a cache miss...
        if self.write_allocate:
            # Find the entry to be evicted.
            victim_idx = plru.get_victim()
            victim_line = cache_set[victim_idx]
            # If we are in write-back mode and the cache line is dirty,
            # it has to be written to the lower level of the memory hierarchy
            # before being overwritten.
            if victim_line.valid and victim_line.dirty and self.write_back:
                victim_addr = ((victim_line.tag * self.num_sets) + index) * self.line_size
                if self.lower:
                    self.lower.write(victim_addr, victim_line.data)
                elif self.memory:
                    self.memory.request(DDRRequest(self.core_id, self.memory.cycle, 'write', victim_addr, value=victim_line.data,num_instr=self.num_instr))

            # The entry is now valid
            victim_line.valid = True
            victim_line.tag = tag
            victim_line.data = val
            # It is dirty (only necessary if write back is active)
            victim_line.dirty = self.write_back
            plru.update_on_access(victim_idx)
        else:
            # If write allocate is false, the data is written to the next level
            # of the memory hierachy.
            if self.lower:
                self.lower.write(addr, val)
            elif self.memory:
                self.memory.request(DDRRequest(self.core_id, self.memory.cycle, 'write', addr, value=val))

        return out
    def stats(self):
        total = self.hits + self.misses
        #print("total", total)
        return {
            "level": self.level,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": self.hits / total if total else 0
        }

# Models full multi-level cache hierarchy for a core
class MultiLevelCache:
    def __init__(self, core_id, l1_conf, l2_conf, l3_conf, interconnect):
        self.core_id = core_id
        self.interconnect = interconnect
        self.l1 = CacheLevel("L1", core_id, memory=None, **l1_conf)
        self.l2 = CacheLevel("L2", core_id, memory=None, **l2_conf)
        # Cache L3 is connected to the DDR memory
        self.l3 = CacheLevel("L3", core_id, memory=interconnect, **l3_conf)
        # Cache L1 is connected to cache L2
        self.l1.lower = self.l2
        # Cache L2 is connected to cache L3
        self.l2.lower = self.l3
        # Cache L3 has no lower level cache.
        self.l3.lower = None

    def read(self, addr, callback):
        out =  self.l1.read(addr, callback)
        if out==None:
            #print("read",out)
            exit()
        return out

    def write(self, addr, val):
        out =  self.l1.write(addr, val)
        if out==None:
            #print("write",out)
            exit()
        return out

    def stats(self):
        return {
            "core": self.core_id,
            "L1": self.l1.stats(),
            "L2": self.l2.stats(),
            "L3": self.l3.stats()
        }
